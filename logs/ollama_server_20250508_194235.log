2025/05/08 19:42:36 routes.go:1158: INFO server config env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/harsh/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: http_proxy: https_proxy: no_proxy:]"
time=2025-05-08T19:42:36.577+05:30 level=INFO source=images.go:754 msg="total blobs: 25"
time=2025-05-08T19:42:36.580+05:30 level=INFO source=images.go:761 msg="total unused blobs removed: 0"
time=2025-05-08T19:42:36.580+05:30 level=INFO source=routes.go:1205 msg="Listening on 127.0.0.1:11434 (version 0.3.14)"
time=2025-05-08T19:42:36.582+05:30 level=INFO source=common.go:135 msg="extracting embedded files" dir=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama3065114570/runners
time=2025-05-08T19:42:36.582+05:30 level=DEBUG source=common.go:168 msg=extracting runner=metal payload=darwin/arm64/metal/ollama_llama_server.gz
time=2025-05-08T19:42:36.602+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama3065114570/runners/metal/ollama_llama_server
time=2025-05-08T19:42:36.602+05:30 level=INFO source=common.go:49 msg="Dynamic LLM libraries" runners=[metal]
time=2025-05-08T19:42:36.602+05:30 level=DEBUG source=common.go:50 msg="Override detection logic by setting OLLAMA_LLM_LIBRARY"
time=2025-05-08T19:42:36.602+05:30 level=DEBUG source=sched.go:105 msg="starting llm scheduler"
time=2025-05-08T19:42:36.689+05:30 level=INFO source=types.go:123 msg="inference compute" id=0 library=metal variant="" compute="" driver=0.0 name="" total="21.3 GiB" available="21.3 GiB"
time=2025-05-08T19:42:52.510+05:30 level=DEBUG source=sched.go:181 msg="updating default concurrency" OLLAMA_MAX_LOADED_MODELS=0x1028316d0 gpu_count=1
time=2025-05-08T19:42:52.527+05:30 level=DEBUG source=sched.go:224 msg="loading first model" model=/Users/harsh/.ollama/models/blobs/sha256-59c55d0e7b6a16d103f324fe4d2cc83843936e1ac65e93b632b6dc577d732630
time=2025-05-08T19:42:52.527+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-05-08T19:42:52.528+05:30 level=DEBUG source=memory.go:170 msg="gpu has too little memory to allocate any layers" id=0 library=metal variant="" compute="" driver=0.0 name="" total="21.3 GiB" available="21.3 GiB" minimum_memory=536870912 layer_size="2.1 GiB" gpu_zer_overhead="0 B" partial_offload="53.3 GiB" full_offload="53.3 GiB"
time=2025-05-08T19:42:52.528+05:30 level=DEBUG source=memory.go:312 msg="insufficient VRAM to load any model layers"
time=2025-05-08T19:42:52.528+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-05-08T19:42:52.528+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-05-08T19:42:52.528+05:30 level=DEBUG source=memory.go:170 msg="gpu has too little memory to allocate any layers" id=0 library=metal variant="" compute="" driver=0.0 name="" total="21.3 GiB" available="21.3 GiB" minimum_memory=536870912 layer_size="2.1 GiB" gpu_zer_overhead="0 B" partial_offload="53.3 GiB" full_offload="53.3 GiB"
time=2025-05-08T19:42:52.528+05:30 level=DEBUG source=memory.go:312 msg="insufficient VRAM to load any model layers"
time=2025-05-08T19:42:52.528+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-05-08T19:42:52.533+05:30 level=INFO source=server.go:105 msg="system memory" total="32.0 GiB" free="9.5 GiB" free_swap="0 B"
time=2025-05-08T19:42:52.533+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-05-08T19:42:52.533+05:30 level=INFO source=memory.go:326 msg="offload to metal" layers.requested=-1 layers.model=41 layers.offload=11 layers.split="" memory.available="[21.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="39.3 GiB" memory.required.partial="21.2 GiB" memory.required.kv="20.0 GiB" memory.required.allocations="[21.2 GiB]" memory.weights.total="24.4 GiB" memory.weights.repeating="24.3 GiB" memory.weights.nonrepeating="157.5 MiB" memory.graph.full="13.3 GiB" memory.graph.partial="13.3 GiB"
time=2025-05-08T19:42:52.533+05:30 level=DEBUG source=common.go:168 msg=extracting runner=metal payload=darwin/arm64/metal/ollama_llama_server.gz
time=2025-05-08T19:42:52.534+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama3065114570/runners/metal/ollama_llama_server
time=2025-05-08T19:42:52.534+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama3065114570/runners/metal/ollama_llama_server
time=2025-05-08T19:42:52.535+05:30 level=INFO source=server.go:388 msg="starting llama server" cmd="/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama3065114570/runners/metal/ollama_llama_server --model /Users/harsh/.ollama/models/blobs/sha256-59c55d0e7b6a16d103f324fe4d2cc83843936e1ac65e93b632b6dc577d732630 --ctx-size 131072 --batch-size 512 --embedding --n-gpu-layers 11 --verbose --threads 8 --no-mmap --parallel 1 --port 63996"
time=2025-05-08T19:42:52.535+05:30 level=DEBUG source=server.go:405 msg=subprocess environment="[PATH=/Users/harsh/Desktop/Continue.dev-Granite-manual-test-cases/granite/bin:/Users/harsh/.pyenv/shims:/Users/harsh/.pyenv/bin:/Users/harsh/.pyenv/bin:/opt/homebrew/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/harsh/.local/bin LD_LIBRARY_PATH=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama3065114570/runners/metal]"
time=2025-05-08T19:42:52.537+05:30 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-05-08T19:42:52.537+05:30 level=INFO source=server.go:587 msg="waiting for llama runner to start responding"
time=2025-05-08T19:42:52.538+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server error"
INFO [main] starting c++ runner | tid="0x1f53f3240" timestamp=1746713573
INFO [main] build info | build=3871 commit="f37ceeaa" tid="0x1f53f3240" timestamp=1746713573
INFO [main] system info | n_threads=8 n_threads_batch=8 system_info="AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="0x1f53f3240" timestamp=1746713573 total_threads=10
INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="9" port="63996" tid="0x1f53f3240" timestamp=1746713573
llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from /Users/harsh/.ollama/models/blobs/sha256-59c55d0e7b6a16d103f324fe4d2cc83843936e1ac65e93b632b6dc577d732630 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = granite
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Granite 3.2 8b Instruct
llama_model_loader: - kv   3:                           general.finetune str              = instruct
llama_model_loader: - kv   4:                           general.basename str              = granite-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = Granite 3.1 8b Instruct
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Ibm Granite
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/ibm-granite/gr...
llama_model_loader: - kv  11:                               general.tags arr[str,3]       = ["language", "granite-3.2", "text-gen...
llama_model_loader: - kv  12:                        granite.block_count u32              = 40
llama_model_loader: - kv  13:                     granite.context_length u32              = 131072
llama_model_loader: - kv  14:                   granite.embedding_length u32              = 4096
llama_model_loader: - kv  15:                granite.feed_forward_length u32              = 12800
llama_model_loader: - kv  16:               granite.attention.head_count u32              = 32
llama_model_loader: - kv  17:            granite.attention.head_count_kv u32              = 8
llama_model_loader: - kv  18:                     granite.rope.freq_base f32              = 10000000.000000
llama_model_loader: - kv  19:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  20:                          general.file_type u32              = 15
llama_model_loader: - kv  21:                         granite.vocab_size u32              = 49155
llama_model_loader: - kv  22:               granite.rope.dimension_count u32              = 128
llama_model_loader: - kv  23:                    granite.attention.scale f32              = 0.007812
llama_model_loader: - kv  24:                    granite.embedding_scale f32              = 12.000000
llama_model_loader: - kv  25:                     granite.residual_scale f32              = 0.220000
llama_model_loader: - kv  26:                        granite.logit_scale f32              = 16.000000
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,49155]   = ["<|end_of_text|>", "<fim_prefix>", "...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,48891]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  34:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  36:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  37:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...
llama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  39:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q4_K:  240 tensors
llama_model_loader: - type q6_K:   41 tensors
time=2025-05-08T19:42:53.549+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.2826 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = granite
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 49155
llm_load_print_meta: n_merges         = 48891
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_layer          = 40
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 1.6e+01
llm_load_print_meta: n_ff             = 12800
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 8.17 B
llm_load_print_meta: model size       = 4.60 GiB (4.84 BPW) 
llm_load_print_meta: general.name     = Granite 3.2 8b Instruct
llm_load_print_meta: BOS token        = 0 '<|end_of_text|>'
llm_load_print_meta: EOS token        = 0 '<|end_of_text|>'
llm_load_print_meta: UNK token        = 0 '<|end_of_text|>'
llm_load_print_meta: PAD token        = 0 '<|end_of_text|>'
llm_load_print_meta: LF token         = 145 'Ä'
llm_load_print_meta: EOG token        = 0 '<|end_of_text|>'
llm_load_print_meta: max token length = 512
llm_load_print_meta: f_embedding_scale = 12.000000
llm_load_print_meta: f_residual_scale  = 0.220000
llm_load_print_meta: f_attention_scale = 0.007812
llm_load_tensors: ggml ctx size =    0.34 MiB
llm_load_tensors: offloading 11 repeating layers to GPU
llm_load_tensors: offloaded 11/41 layers to GPU
llm_load_tensors:        CPU buffer size =  3596.30 MiB
llm_load_tensors:      Metal buffer size =  1273.42 MiB
time=2025-05-08T19:42:53.800+05:30 level=DEBUG source=server.go:632 msg="model load progress 0.11"
time=2025-05-08T19:42:54.051+05:30 level=DEBUG source=server.go:632 msg="model load progress 0.24"
time=2025-05-08T19:42:54.302+05:30 level=DEBUG source=server.go:632 msg="model load progress 0.35"
time=2025-05-08T19:42:54.553+05:30 level=DEBUG source=server.go:632 msg="model load progress 0.43"
time=2025-05-08T19:42:54.805+05:30 level=DEBUG source=server.go:632 msg="model load progress 0.49"
time=2025-05-08T19:42:55.056+05:30 level=DEBUG source=server.go:632 msg="model load progress 0.56"
time=2025-05-08T19:42:55.307+05:30 level=DEBUG source=server.go:632 msg="model load progress 0.66"
time=2025-05-08T19:42:55.558+05:30 level=DEBUG source=server.go:632 msg="model load progress 0.77"
time=2025-05-08T19:42:55.809+05:30 level=DEBUG source=server.go:632 msg="model load progress 0.83"
time=2025-05-08T19:42:56.061+05:30 level=DEBUG source=server.go:632 msg="model load progress 0.93"
llama_new_context_with_model: n_ctx      = 131072
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000000.0
llama_new_context_with_model: freq_scale = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M1 Pro
ggml_metal_init: picking default device: Apple M1 Pro
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M1 Pro
ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction support   = true
ggml_metal_init: simdgroup matrix mul. support = true
ggml_metal_init: hasUnifiedMemory              = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB
time=2025-05-08T19:42:56.312+05:30 level=DEBUG source=server.go:632 msg="model load progress 1.00"
time=2025-05-08T19:42:56.564+05:30 level=DEBUG source=server.go:635 msg="model load completed, waiting for server to become available" status="llm server loading model"
llama_kv_cache_init:        CPU KV buffer size = 14848.00 MiB
llama_kv_cache_init:      Metal KV buffer size =  5632.00 MiB
llama_new_context_with_model: KV self size  = 20480.00 MiB, K (f16): 10240.00 MiB, V (f16): 10240.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.20 MiB
llama_new_context_with_model:      Metal compute buffer size =  8480.00 MiB
llama_new_context_with_model:        CPU compute buffer size =  8480.01 MiB
llama_new_context_with_model: graph nodes  = 1368
llama_new_context_with_model: graph splits = 468
DEBUG [initialize] initializing slots | n_slots=1 tid="0x1f53f3240" timestamp=1746713588
DEBUG [initialize] new slot | n_ctx_slot=131072 slot_id=0 tid="0x1f53f3240" timestamp=1746713588
INFO [main] model loaded | tid="0x1f53f3240" timestamp=1746713588
DEBUG [update_slots] all slots are idle and system prompt is empty, clear the KV cache | tid="0x1f53f3240" timestamp=1746713588
time=2025-05-08T19:43:08.665+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server not responding"
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=0 tid="0x1f53f3240" timestamp=1746713591
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=1 tid="0x1f53f3240" timestamp=1746713591
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=2 tid="0x1f53f3240" timestamp=1746713591
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=3 tid="0x1f53f3240" timestamp=1746713591
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=4 tid="0x1f53f3240" timestamp=1746713591
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=5 tid="0x1f53f3240" timestamp=1746713591
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=6 tid="0x1f53f3240" timestamp=1746713591
time=2025-05-08T19:43:11.180+05:30 level=INFO source=server.go:626 msg="llama runner started in 18.64 seconds"
time=2025-05-08T19:43:11.180+05:30 level=DEBUG source=sched.go:462 msg="finished setting up runner" model=/Users/harsh/.ollama/models/blobs/sha256-59c55d0e7b6a16d103f324fe4d2cc83843936e1ac65e93b632b6dc577d732630
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=7 tid="0x1f53f3240" timestamp=1746713591
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=64020 status=200 tid="0x16b63b000" timestamp=1746713591
time=2025-05-08T19:43:11.205+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|>Always include the language and file name in the info string when you write code blocks, for example '```python file.py'.\n\nI am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>answer brief: What is AWS?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=8 tid="0x1f53f3240" timestamp=1746713591
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=9 tid="0x1f53f3240" timestamp=1746713591
DEBUG [update_slots] slot progression | ga_i=0 n_past=0 n_past_se=0 n_prompt_tokens_processed=128 slot_id=0 task_id=9 tid="0x1f53f3240" timestamp=1746713591
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=9 tid="0x1f53f3240" timestamp=1746713591
DEBUG [print_timings] prompt eval time     =    6068.13 ms /   128 tokens (   47.41 ms per token,    21.09 tokens per second) | n_prompt_tokens_processed=128 n_tokens_second=21.093813085744703 slot_id=0 t_prompt_processing=6068.13 t_token=47.407265625 task_id=9 tid="0x1f53f3240" timestamp=1746713603
DEBUG [print_timings] generation eval time =    6276.47 ms /    80 runs   (   78.46 ms per token,    12.75 tokens per second) | n_decoded=80 n_tokens_second=12.746011972010397 slot_id=0 t_token=78.4559125 t_token_generation=6276.473 task_id=9 tid="0x1f53f3240" timestamp=1746713603
DEBUG [print_timings]           total time =   12344.60 ms | slot_id=0 t_prompt_processing=6068.13 t_token_generation=6276.473 t_total=12344.603 task_id=9 tid="0x1f53f3240" timestamp=1746713603
DEBUG [update_slots] slot released | n_cache_tokens=208 n_ctx=131072 n_past=207 n_system_tokens=0 slot_id=0 task_id=9 tid="0x1f53f3240" timestamp=1746713603 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=64020 status=200 tid="0x16b63b000" timestamp=1746713603
[GIN] 2025/05/08 - 19:43:23 | 200 | 31.090057875s |       127.0.0.1 | POST     "/api/chat"
time=2025-05-08T19:43:23.553+05:30 level=DEBUG source=sched.go:466 msg="context for request finished"
time=2025-05-08T19:43:23.553+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-59c55d0e7b6a16d103f324fe4d2cc83843936e1ac65e93b632b6dc577d732630 duration=30m0s
time=2025-05-08T19:43:23.553+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-59c55d0e7b6a16d103f324fe4d2cc83843936e1ac65e93b632b6dc577d732630 refCount=0
[GIN] 2025/05/08 - 19:43:23 | 200 |     482.792µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/05/08 - 19:43:23 | 200 |         135µs |       127.0.0.1 | GET      "/api/ps"
time=2025-05-08T19:43:28.498+05:30 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/Users/harsh/.ollama/models/blobs/sha256-59c55d0e7b6a16d103f324fe4d2cc83843936e1ac65e93b632b6dc577d732630
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=92 tid="0x1f53f3240" timestamp=1746713608
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=93 tid="0x1f53f3240" timestamp=1746713608
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=64029 status=200 tid="0x16b1db000" timestamp=1746713608
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=94 tid="0x1f53f3240" timestamp=1746713608
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=64029 status=200 tid="0x16b1db000" timestamp=1746713608
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=95 tid="0x1f53f3240" timestamp=1746713608
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=64030 status=200 tid="0x16b267000" timestamp=1746713608
time=2025-05-08T19:43:28.507+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|>Always include the language and file name in the info string when you write code blocks, for example '```python file.py'.\n\nI am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>answer brief: What is AWS?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>```python\n# file: aws_definition.py\n\ndef what_is_aws():\n    return \"AWS (Amazon Web Services) is a comprehensive, evolving cloud computing platform provided by Amazon. It offers a mix of infrastructure as a service (IaaS), platform as a service (PaaS), and packaged software as a service (SaaS) offerings.\"\n```<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>what is java<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=96 tid="0x1f53f3240" timestamp=1746713608
DEBUG [prefix_slot] slot with common prefix found | 0=["slot_id",0,"characters",704]
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=97 tid="0x1f53f3240" timestamp=1746713608
DEBUG [update_slots] slot progression | ga_i=0 n_past=139 n_past_se=0 n_prompt_tokens_processed=220 slot_id=0 task_id=97 tid="0x1f53f3240" timestamp=1746713608
DEBUG [update_slots] kv cache rm [p0, end) | p0=139 slot_id=0 task_id=97 tid="0x1f53f3240" timestamp=1746713608
DEBUG [print_timings] prompt eval time     =    2566.75 ms /   220 tokens (   11.67 ms per token,    85.71 tokens per second) | n_prompt_tokens_processed=220 n_tokens_second=85.7114694802885 slot_id=0 t_prompt_processing=2566.751 t_token=11.667050000000001 task_id=97 tid="0x1f53f3240" timestamp=1746713619
DEBUG [print_timings] generation eval time =    8916.81 ms /    95 runs   (   93.86 ms per token,    10.65 tokens per second) | n_decoded=95 n_tokens_second=10.654033151538146 slot_id=0 t_token=93.86116842105262 t_token_generation=8916.811 task_id=97 tid="0x1f53f3240" timestamp=1746713619
DEBUG [print_timings]           total time =   11483.56 ms | slot_id=0 t_prompt_processing=2566.751 t_token_generation=8916.811 t_total=11483.562 task_id=97 tid="0x1f53f3240" timestamp=1746713619
DEBUG [update_slots] slot released | n_cache_tokens=315 n_ctx=131072 n_past=314 n_system_tokens=0 slot_id=0 task_id=97 tid="0x1f53f3240" timestamp=1746713619 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=64030 status=200 tid="0x16b267000" timestamp=1746713619
[GIN] 2025/05/08 - 19:43:39 | 200 | 11.501729708s |       127.0.0.1 | POST     "/api/chat"
time=2025-05-08T19:43:39.992+05:30 level=DEBUG source=sched.go:407 msg="context for request finished"
time=2025-05-08T19:43:39.992+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-59c55d0e7b6a16d103f324fe4d2cc83843936e1ac65e93b632b6dc577d732630 duration=30m0s
time=2025-05-08T19:43:39.992+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-59c55d0e7b6a16d103f324fe4d2cc83843936e1ac65e93b632b6dc577d732630 refCount=0
