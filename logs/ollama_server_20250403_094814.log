2025/04/03 09:48:14 routes.go:1158: INFO server config env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/harsh/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: http_proxy: https_proxy: no_proxy:]"
time=2025-04-03T09:48:14.646+05:30 level=INFO source=images.go:754 msg="total blobs: 25"
time=2025-04-03T09:48:14.649+05:30 level=INFO source=images.go:761 msg="total unused blobs removed: 0"
time=2025-04-03T09:48:14.650+05:30 level=INFO source=routes.go:1205 msg="Listening on 127.0.0.1:11434 (version 0.3.14)"
time=2025-04-03T09:48:14.657+05:30 level=INFO source=common.go:135 msg="extracting embedded files" dir=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama526949391/runners
time=2025-04-03T09:48:14.658+05:30 level=DEBUG source=common.go:168 msg=extracting runner=metal payload=darwin/arm64/metal/ollama_llama_server.gz
time=2025-04-03T09:48:14.679+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama526949391/runners/metal/ollama_llama_server
time=2025-04-03T09:48:14.680+05:30 level=INFO source=common.go:49 msg="Dynamic LLM libraries" runners=[metal]
time=2025-04-03T09:48:14.680+05:30 level=DEBUG source=common.go:50 msg="Override detection logic by setting OLLAMA_LLM_LIBRARY"
time=2025-04-03T09:48:14.680+05:30 level=DEBUG source=sched.go:105 msg="starting llm scheduler"
time=2025-04-03T09:48:14.735+05:30 level=INFO source=types.go:123 msg="inference compute" id=0 library=metal variant="" compute="" driver=0.0 name="" total="21.3 GiB" available="21.3 GiB"
time=2025-04-03T09:48:32.332+05:30 level=DEBUG source=sched.go:181 msg="updating default concurrency" OLLAMA_MAX_LOADED_MODELS=0x104a2d6d0 gpu_count=1
time=2025-04-03T09:48:32.341+05:30 level=DEBUG source=sched.go:224 msg="loading first model" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
time=2025-04-03T09:48:32.341+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-04-03T09:48:32.342+05:30 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 gpu=0 parallel=4 available=22906503168 required="6.2 GiB"
time=2025-04-03T09:48:32.346+05:30 level=INFO source=server.go:105 msg="system memory" total="32.0 GiB" free="10.8 GiB" free_swap="0 B"
time=2025-04-03T09:48:32.346+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-04-03T09:48:32.346+05:30 level=INFO source=memory.go:326 msg="offload to metal" layers.requested=-1 layers.model=41 layers.offload=41 layers.split="" memory.available="[21.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.2 GiB" memory.required.partial="6.2 GiB" memory.required.kv="2.5 GiB" memory.required.allocations="[6.2 GiB]" memory.weights.total="3.8 GiB" memory.weights.repeating="3.8 GiB" memory.weights.nonrepeating="78.8 MiB" memory.graph.full="1.6 GiB" memory.graph.partial="1.6 GiB"
time=2025-04-03T09:48:32.346+05:30 level=DEBUG source=common.go:168 msg=extracting runner=metal payload=darwin/arm64/metal/ollama_llama_server.gz
time=2025-04-03T09:48:32.346+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama526949391/runners/metal/ollama_llama_server
time=2025-04-03T09:48:32.346+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama526949391/runners/metal/ollama_llama_server
time=2025-04-03T09:48:32.348+05:30 level=INFO source=server.go:388 msg="starting llama server" cmd="/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama526949391/runners/metal/ollama_llama_server --model /Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 --ctx-size 32384 --batch-size 512 --embedding --n-gpu-layers 41 --verbose --threads 8 --parallel 4 --port 62594"
time=2025-04-03T09:48:32.348+05:30 level=DEBUG source=server.go:405 msg=subprocess environment="[PATH=/Users/harsh/Desktop/Continue.dev-Granite-manual-test-cases/granite/bin:/opt/anaconda3/bin:/opt/homebrew/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/harsh/.local/bin LD_LIBRARY_PATH=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama526949391/runners/metal]"
time=2025-04-03T09:48:32.349+05:30 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-04-03T09:48:32.349+05:30 level=INFO source=server.go:587 msg="waiting for llama runner to start responding"
time=2025-04-03T09:48:32.350+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server error"
INFO [main] starting c++ runner | tid="0x1e9db3240" timestamp=1743653912
INFO [main] build info | build=3871 commit="f37ceeaa" tid="0x1e9db3240" timestamp=1743653912
INFO [main] system info | n_threads=8 n_threads_batch=8 system_info="AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="0x1e9db3240" timestamp=1743653912 total_threads=10
INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="9" port="62594" tid="0x1e9db3240" timestamp=1743653912
llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from /Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = granite
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Granite 3.2 2b Instruct
llama_model_loader: - kv   3:                           general.finetune str              = instruct
llama_model_loader: - kv   4:                           general.basename str              = granite-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 2B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = Granite 3.1 2b Instruct
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Ibm Granite
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/ibm-granite/gr...
llama_model_loader: - kv  11:                               general.tags arr[str,3]       = ["language", "granite-3.2", "text-gen...
llama_model_loader: - kv  12:                        granite.block_count u32              = 40
llama_model_loader: - kv  13:                     granite.context_length u32              = 131072
llama_model_loader: - kv  14:                   granite.embedding_length u32              = 2048
llama_model_loader: - kv  15:                granite.feed_forward_length u32              = 8192
llama_model_loader: - kv  16:               granite.attention.head_count u32              = 32
llama_model_loader: - kv  17:            granite.attention.head_count_kv u32              = 8
llama_model_loader: - kv  18:                     granite.rope.freq_base f32              = 5000000.000000
llama_model_loader: - kv  19:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  20:                          general.file_type u32              = 15
llama_model_loader: - kv  21:                         granite.vocab_size u32              = 49155
llama_model_loader: - kv  22:               granite.rope.dimension_count u32              = 64
llama_model_loader: - kv  23:                    granite.attention.scale f32              = 0.015625
llama_model_loader: - kv  24:                    granite.embedding_scale f32              = 12.000000
llama_model_loader: - kv  25:                     granite.residual_scale f32              = 0.220000
llama_model_loader: - kv  26:                        granite.logit_scale f32              = 8.000000
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,49155]   = ["<|end_of_text|>", "<fim_prefix>", "...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,48891]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  34:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  36:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  37:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...
llama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  39:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q4_K:  240 tensors
llama_model_loader: - type q6_K:   41 tensors
time=2025-04-03T09:48:32.854+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.2826 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = granite
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 49155
llm_load_print_meta: n_merges         = 48891
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 40
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 512
llm_load_print_meta: n_embd_v_gqa     = 512
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 8.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 5000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 2.53 B
llm_load_print_meta: model size       = 1.44 GiB (4.87 BPW) 
llm_load_print_meta: general.name     = Granite 3.2 2b Instruct
llm_load_print_meta: BOS token        = 0 '<|end_of_text|>'
llm_load_print_meta: EOS token        = 0 '<|end_of_text|>'
llm_load_print_meta: UNK token        = 0 '<|end_of_text|>'
llm_load_print_meta: PAD token        = 0 '<|end_of_text|>'
llm_load_print_meta: LF token         = 145 'Ä'
llm_load_print_meta: EOG token        = 0 '<|end_of_text|>'
llm_load_print_meta: max token length = 512
llm_load_print_meta: f_embedding_scale = 12.000000
llm_load_print_meta: f_residual_scale  = 0.220000
llm_load_print_meta: f_attention_scale = 0.015625
llm_load_tensors: ggml ctx size =    0.34 MiB
ggml_backend_metal_log_allocated_size: allocated buffer, size =  1472.06 MiB, ( 1472.12 / 21845.34)
llm_load_tensors: offloading 40 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 41/41 layers to GPU
llm_load_tensors:        CPU buffer size =    78.75 MiB
llm_load_tensors:      Metal buffer size =  1472.05 MiB
llama_new_context_with_model: n_ctx      = 32384
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 5000000.0
llama_new_context_with_model: freq_scale = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M1 Pro
ggml_metal_init: picking default device: Apple M1 Pro
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M1 Pro
ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction support   = true
ggml_metal_init: simdgroup matrix mul. support = true
ggml_metal_init: hasUnifiedMemory              = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB
time=2025-04-03T09:48:33.106+05:30 level=DEBUG source=server.go:632 msg="model load progress 1.00"
time=2025-04-03T09:48:33.357+05:30 level=DEBUG source=server.go:635 msg="model load completed, waiting for server to become available" status="llm server loading model"
llama_kv_cache_init:      Metal KV buffer size =  2530.00 MiB
llama_new_context_with_model: KV self size  = 2530.00 MiB, K (f16): 1265.00 MiB, V (f16): 1265.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.78 MiB
llama_new_context_with_model:      Metal compute buffer size =  2103.25 MiB
llama_new_context_with_model:        CPU compute buffer size =    67.26 MiB
llama_new_context_with_model: graph nodes  = 1368
llama_new_context_with_model: graph splits = 2
DEBUG [initialize] initializing slots | n_slots=4 tid="0x1e9db3240" timestamp=1743653915
DEBUG [initialize] new slot | n_ctx_slot=8096 slot_id=0 tid="0x1e9db3240" timestamp=1743653915
DEBUG [initialize] new slot | n_ctx_slot=8096 slot_id=1 tid="0x1e9db3240" timestamp=1743653915
DEBUG [initialize] new slot | n_ctx_slot=8096 slot_id=2 tid="0x1e9db3240" timestamp=1743653915
DEBUG [initialize] new slot | n_ctx_slot=8096 slot_id=3 tid="0x1e9db3240" timestamp=1743653915
INFO [main] model loaded | tid="0x1e9db3240" timestamp=1743653915
DEBUG [update_slots] all slots are idle and system prompt is empty, clear the KV cache | tid="0x1e9db3240" timestamp=1743653915
time=2025-04-03T09:48:35.574+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server not responding"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=0 tid="0x1e9db3240" timestamp=1743653915
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=1 tid="0x1e9db3240" timestamp=1743653915
time=2025-04-03T09:48:35.827+05:30 level=INFO source=server.go:626 msg="llama runner started in 3.48 seconds"
time=2025-04-03T09:48:35.827+05:30 level=DEBUG source=sched.go:462 msg="finished setting up runner" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=2 tid="0x1e9db3240" timestamp=1743653915
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=62601 status=200 tid="0x16f293000" timestamp=1743653915
time=2025-04-03T09:48:35.833+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|>Always include the language and file name in the info string when you write code blocks, for example '```python file.py'.\n\nI am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>lanswer brief: What is AWS?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=3 tid="0x1e9db3240" timestamp=1743653915
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=4 tid="0x1e9db3240" timestamp=1743653915
DEBUG [update_slots] slot progression | ga_i=0 n_past=0 n_past_se=0 n_prompt_tokens_processed=129 slot_id=0 task_id=4 tid="0x1e9db3240" timestamp=1743653915
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=4 tid="0x1e9db3240" timestamp=1743653915
DEBUG [print_timings] prompt eval time     =     298.17 ms /   129 tokens (    2.31 ms per token,   432.64 tokens per second) | n_prompt_tokens_processed=129 n_tokens_second=432.63764752440716 slot_id=0 t_prompt_processing=298.171 t_token=2.311403100775194 task_id=4 tid="0x1e9db3240" timestamp=1743653918
DEBUG [print_timings] generation eval time =    2710.50 ms /   137 runs   (   19.78 ms per token,    50.54 tokens per second) | n_decoded=137 n_tokens_second=50.544198688138245 slot_id=0 t_token=19.784664233576642 t_token_generation=2710.499 task_id=4 tid="0x1e9db3240" timestamp=1743653918
DEBUG [print_timings]           total time =    3008.67 ms | slot_id=0 t_prompt_processing=298.171 t_token_generation=2710.499 t_total=3008.6699999999996 task_id=4 tid="0x1e9db3240" timestamp=1743653918
DEBUG [update_slots] slot released | n_cache_tokens=266 n_ctx=32384 n_past=265 n_system_tokens=0 slot_id=0 task_id=4 tid="0x1e9db3240" timestamp=1743653918 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=62601 status=200 tid="0x16f293000" timestamp=1743653918
[GIN] 2025/04/03 - 09:48:38 | 200 |  6.526609875s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-03T09:48:38.843+05:30 level=DEBUG source=sched.go:466 msg="context for request finished"
time=2025-04-03T09:48:38.843+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 duration=30m0s
time=2025-04-03T09:48:38.844+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 refCount=0
time=2025-04-03T09:48:38.902+05:30 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=144 tid="0x1e9db3240" timestamp=1743653918
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=145 tid="0x1e9db3240" timestamp=1743653918
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=62603 status=200 tid="0x16f31f000" timestamp=1743653918
time=2025-04-03T09:48:38.906+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|>I am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Given the following... please reply with a title for the chat that is 3-4 words in length, all words used should be directly related to the content of the chat, avoid using verbs unless they are directly related to the content of the chat, no additional text or explanation, you don't need ending punctuation.\n\nAWS (Amazon Web Services) is a comprehensive, evolving cloud computing platform provided by Amazon. It offers a mix of infrastructure as a service (IaaS), platform as a service (PaaS), and packaged software services known as managed services. These include compute power for hosting applications; storage solutions; networking capabilities to connect your resources; analytics tools for data analysis; machine learning services for AI development, and more. AWS operates on an \"everything-as-a-service\" model where you can rent computing resources, such as servers or storage, rather than buying them. It has data centers around the world to ensure low latency and high availability of its services.<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=146 tid="0x1e9db3240" timestamp=1743653918
DEBUG [prefix_slot] slot with common prefix found | 0=["slot_id",0,"characters",38]
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=147 tid="0x1e9db3240" timestamp=1743653918
DEBUG [update_slots] slot progression | ga_i=0 n_past=3 n_past_se=0 n_prompt_tokens_processed=296 slot_id=0 task_id=147 tid="0x1e9db3240" timestamp=1743653918
DEBUG [update_slots] kv cache rm [p0, end) | p0=3 slot_id=0 task_id=147 tid="0x1e9db3240" timestamp=1743653918
DEBUG [print_timings] prompt eval time     =     558.49 ms /   296 tokens (    1.89 ms per token,   530.00 tokens per second) | n_prompt_tokens_processed=296 n_tokens_second=530.0005371627066 slot_id=0 t_prompt_processing=558.49 t_token=1.8867905405405405 task_id=147 tid="0x1e9db3240" timestamp=1743653919
DEBUG [print_timings] generation eval time =     219.43 ms /    12 runs   (   18.29 ms per token,    54.69 tokens per second) | n_decoded=12 n_tokens_second=54.68664552116374 slot_id=0 t_token=18.285999999999998 t_token_generation=219.432 task_id=147 tid="0x1e9db3240" timestamp=1743653919
DEBUG [print_timings]           total time =     777.92 ms | slot_id=0 t_prompt_processing=558.49 t_token_generation=219.432 t_total=777.922 task_id=147 tid="0x1e9db3240" timestamp=1743653919
DEBUG [update_slots] slot released | n_cache_tokens=308 n_ctx=32384 n_past=307 n_system_tokens=0 slot_id=0 task_id=147 tid="0x1e9db3240" timestamp=1743653919 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=62603 status=200 tid="0x16f31f000" timestamp=1743653919
[GIN] 2025/04/03 - 09:48:39 | 200 |   790.26025ms |       127.0.0.1 | POST     "/api/chat"
time=2025-04-03T09:48:39.686+05:30 level=DEBUG source=sched.go:407 msg="context for request finished"
time=2025-04-03T09:48:39.686+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 duration=30m0s
time=2025-04-03T09:48:39.686+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 refCount=0
[GIN] 2025/04/03 - 09:48:52 | 200 |      73.417µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/03 - 09:48:52 | 200 |      86.125µs |       127.0.0.1 | GET      "/api/ps"
time=2025-04-03T09:48:58.348+05:30 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=162 tid="0x1e9db3240" timestamp=1743653938
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=163 tid="0x1e9db3240" timestamp=1743653938
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=62609 status=200 tid="0x16f3ab000" timestamp=1743653938
time=2025-04-03T09:48:58.351+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|>Always include the language and file name in the info string when you write code blocks, for example '```python file.py'.\n\nI am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>what is java<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=164 tid="0x1e9db3240" timestamp=1743653938
DEBUG [prefix_slot] slot with common prefix found | 0=["slot_id",0,"characters",38]
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=165 tid="0x1e9db3240" timestamp=1743653938
DEBUG [update_slots] slot progression | ga_i=0 n_past=3 n_past_se=0 n_prompt_tokens_processed=124 slot_id=0 task_id=165 tid="0x1e9db3240" timestamp=1743653938
DEBUG [update_slots] kv cache rm [p0, end) | p0=3 slot_id=0 task_id=165 tid="0x1e9db3240" timestamp=1743653938
DEBUG [print_timings] prompt eval time     =     548.75 ms /   124 tokens (    4.43 ms per token,   225.97 tokens per second) | n_prompt_tokens_processed=124 n_tokens_second=225.96810933940773 slot_id=0 t_prompt_processing=548.75 t_token=4.425403225806452 task_id=165 tid="0x1e9db3240" timestamp=1743653949
DEBUG [print_timings] generation eval time =   11032.18 ms /   533 runs   (   20.70 ms per token,    48.31 tokens per second) | n_decoded=533 n_tokens_second=48.313194224570054 slot_id=0 t_token=20.698279549718574 t_token_generation=11032.183 task_id=165 tid="0x1e9db3240" timestamp=1743653949
DEBUG [print_timings]           total time =   11580.93 ms | slot_id=0 t_prompt_processing=548.75 t_token_generation=11032.183 t_total=11580.933 task_id=165 tid="0x1e9db3240" timestamp=1743653949
DEBUG [update_slots] slot released | n_cache_tokens=657 n_ctx=32384 n_past=656 n_system_tokens=0 slot_id=0 task_id=165 tid="0x1e9db3240" timestamp=1743653949 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=62609 status=200 tid="0x16f3ab000" timestamp=1743653949
[GIN] 2025/04/03 - 09:49:09 | 200 | 11.594505375s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-03T09:49:09.933+05:30 level=DEBUG source=sched.go:407 msg="context for request finished"
time=2025-04-03T09:49:09.933+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 duration=30m0s
time=2025-04-03T09:49:09.933+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 refCount=0
time=2025-04-03T09:49:10.041+05:30 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=701 tid="0x1e9db3240" timestamp=1743653950
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=702 tid="0x1e9db3240" timestamp=1743653950
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=62612 status=200 tid="0x16f437000" timestamp=1743653950
time=2025-04-03T09:49:10.047+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|>I am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Given the following... please reply with a title for the chat that is 3-4 words in length, all words used should be directly related to the content of the chat, avoid using verbs unless they are directly related to the content of the chat, no additional text or explanation, you don't need ending punctuation.\n\nJava is a high-level, class-based, object-oriented programming language designed by James Gosling at Sun Microsystems (now owned by Oracle Corporation) in the mid-1990s. It's one of the most widely used programming languages and known for its \"write once, run anywhere\" motto – this means that compiled Java code can run on all platforms that support Java without needing recompilation.\n\nJava is often utilized for developing a broad range of applications:\n\n1. **Desktop Applications**: With Java's AWT (Abstract Window Toolkit) or Swing libraries, developers can build graphical user interfaces (GUIs).\n2. **Web Applications**: The most common use case for Java is in server-side programming with the Java Enterprise Edition (EE), which forms the core of Java Platform, Enterprise Edition (Java EE). It's backed by Servlets and JavaServer Pages (JSP) technologies.\n3. **Mobile Apps**: Android applications are primarily developed using Java along with its associated framework for building user interfaces—Android SDK.\n4. **Enterprise Software Systems**: Java is often used in enterprise environments due to its robustness, reliability, and security features. Frameworks like Spring, Hibernate, and Struts support this use case.\n5. **Big Data Technologies (with the help of Apache Spark)**: Libraries such as Apache Spark's RDD or DataFrame API allow Java developers to handle big data processing tasks efficiently.\n6. **Scripting and Systems Management**: Java can also be used for scripting purposes, often in conjunction with tools like Jenkins or Ansible for systems management and automation.\n\nJava's syntax is clean and close to that of C and C++, making it easy for developers whose background is in these languages to pick up Java quickly. It benefits from a strong ecosystem, including libraries (like Apache Commons), build tools (Maven, Gradle), testing frameworks (JUnit), and more, all contributing to its versatility as a general-purpose programming language.\n\nHere's a simple \"Hello, World!\" program written in Java:\n\n\n\nTo run this code, you'd use a Java compiler (like `javac` command in the terminal), followed by a JVM (Java Virtual Machine) to execute it. For example:<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=703 tid="0x1e9db3240" timestamp=1743653950
DEBUG [prefix_slot] slot with common prefix found | 0=["slot_id",0,"characters",38]
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=704 tid="0x1e9db3240" timestamp=1743653950
DEBUG [update_slots] slot progression | ga_i=0 n_past=3 n_past_se=0 n_prompt_tokens_processed=620 slot_id=0 task_id=704 tid="0x1e9db3240" timestamp=1743653950
DEBUG [update_slots] kv cache rm [p0, end) | p0=3 slot_id=0 task_id=704 tid="0x1e9db3240" timestamp=1743653950
DEBUG [print_timings] prompt eval time     =    1162.94 ms /   620 tokens (    1.88 ms per token,   533.13 tokens per second) | n_prompt_tokens_processed=620 n_tokens_second=533.1301706102535 slot_id=0 t_prompt_processing=1162.943 t_token=1.8757145161290323 task_id=704 tid="0x1e9db3240" timestamp=1743653951
DEBUG [print_timings] generation eval time =     238.66 ms /    12 runs   (   19.89 ms per token,    50.28 tokens per second) | n_decoded=12 n_tokens_second=50.28178751754625 slot_id=0 t_token=19.887916666666666 t_token_generation=238.655 task_id=704 tid="0x1e9db3240" timestamp=1743653951
DEBUG [print_timings]           total time =    1401.60 ms | slot_id=0 t_prompt_processing=1162.943 t_token_generation=238.655 t_total=1401.598 task_id=704 tid="0x1e9db3240" timestamp=1743653951
DEBUG [update_slots] slot released | n_cache_tokens=632 n_ctx=32384 n_past=631 n_system_tokens=0 slot_id=0 task_id=704 tid="0x1e9db3240" timestamp=1743653951 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=62612 status=200 tid="0x16f437000" timestamp=1743653951
[GIN] 2025/04/03 - 09:49:11 | 200 |  1.415749333s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-03T09:49:11.450+05:30 level=DEBUG source=sched.go:407 msg="context for request finished"
time=2025-04-03T09:49:11.450+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 duration=30m0s
time=2025-04-03T09:49:11.450+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 refCount=0
time=2025-04-03T09:49:43.193+05:30 level=DEBUG source=sched.go:496 msg="gpu reported" gpu=0 library=metal available="21.3 GiB"
time=2025-04-03T09:49:43.193+05:30 level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=0 library=metal total="21.3 GiB" available="15.2 GiB"
time=2025-04-03T09:49:43.193+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[15.2 GiB]"
time=2025-04-03T09:49:43.193+05:30 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=0 parallel=1 available=16299442598 required="864.9 MiB"
time=2025-04-03T09:49:43.193+05:30 level=DEBUG source=sched.go:249 msg="new model fits with existing models, loading"
time=2025-04-03T09:49:43.195+05:30 level=INFO source=server.go:105 msg="system memory" total="32.0 GiB" free="8.4 GiB" free_swap="0 B"
time=2025-04-03T09:49:43.195+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[15.2 GiB]"
time=2025-04-03T09:49:43.195+05:30 level=INFO source=memory.go:326 msg="offload to metal" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[15.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="864.9 MiB" memory.required.partial="864.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[864.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-04-03T09:49:43.195+05:30 level=DEBUG source=common.go:168 msg=extracting runner=metal payload=darwin/arm64/metal/ollama_llama_server.gz
time=2025-04-03T09:49:43.196+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama526949391/runners/metal/ollama_llama_server
time=2025-04-03T09:49:43.196+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama526949391/runners/metal/ollama_llama_server
time=2025-04-03T09:49:43.196+05:30 level=INFO source=server.go:388 msg="starting llama server" cmd="/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama526949391/runners/metal/ollama_llama_server --model /Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --embedding --n-gpu-layers 13 --verbose --threads 8 --parallel 1 --port 62616"
time=2025-04-03T09:49:43.197+05:30 level=DEBUG source=server.go:405 msg=subprocess environment="[PATH=/Users/harsh/Desktop/Continue.dev-Granite-manual-test-cases/granite/bin:/opt/anaconda3/bin:/opt/homebrew/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/harsh/.local/bin LD_LIBRARY_PATH=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama526949391/runners/metal]"
time=2025-04-03T09:49:43.198+05:30 level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-04-03T09:49:43.198+05:30 level=INFO source=server.go:587 msg="waiting for llama runner to start responding"
time=2025-04-03T09:49:43.199+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server error"
INFO [main] starting c++ runner | tid="0x1e9db3240" timestamp=1743653983
INFO [main] build info | build=3871 commit="f37ceeaa" tid="0x1e9db3240" timestamp=1743653983
INFO [main] system info | n_threads=8 n_threads_batch=8 system_info="AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="0x1e9db3240" timestamp=1743653983 total_threads=10
INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="9" port="62616" tid="0x1e9db3240" timestamp=1743653983
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: ggml ctx size =    0.10 MiB
ggml_backend_metal_log_allocated_size: allocated buffer, size =   260.88 MiB, (  260.94 / 21845.34)
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CPU buffer size =    44.72 MiB
llm_load_tensors:      Metal buffer size =   260.87 MiB
llama_new_context_with_model: n_ctx      = 8192
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 1000.0
llama_new_context_with_model: freq_scale = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M1 Pro
ggml_metal_init: picking default device: Apple M1 Pro
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M1 Pro
ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction support   = true
ggml_metal_init: simdgroup matrix mul. support = true
ggml_metal_init: hasUnifiedMemory              = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB
llama_kv_cache_init:      Metal KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
llama_new_context_with_model:      Metal compute buffer size =    23.00 MiB
llama_new_context_with_model:        CPU compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 2
time=2025-04-03T09:49:43.451+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server loading model"
time=2025-04-03T09:49:43.451+05:30 level=DEBUG source=server.go:632 msg="model load progress 1.00"
DEBUG [initialize] initializing slots | n_slots=1 tid="0x1e9db3240" timestamp=1743653983
DEBUG [initialize] new slot | n_ctx_slot=8192 slot_id=0 tid="0x1e9db3240" timestamp=1743653983
INFO [main] model loaded | tid="0x1e9db3240" timestamp=1743653983
DEBUG [update_slots] all slots are idle and system prompt is empty, clear the KV cache | tid="0x1e9db3240" timestamp=1743653983
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=0 tid="0x1e9db3240" timestamp=1743653983
time=2025-04-03T09:49:43.703+05:30 level=INFO source=server.go:626 msg="llama runner started in 0.50 seconds"
time=2025-04-03T09:49:43.703+05:30 level=DEBUG source=sched.go:462 msg="finished setting up runner" model=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=1 tid="0x1e9db3240" timestamp=1743653983
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=62618 status=200 tid="0x16f927000" timestamp=1743653983
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=2 tid="0x1e9db3240" timestamp=1743653983
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=62619 status=200 tid="0x16f9b3000" timestamp=1743653983
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=3 tid="0x1e9db3240" timestamp=1743653983
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=62619 status=200 tid="0x16f9b3000" timestamp=1743653983
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=4 tid="0x1e9db3240" timestamp=1743653983
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=5 tid="0x1e9db3240" timestamp=1743653983
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=5 tid="0x1e9db3240" timestamp=1743653983
DEBUG [update_slots] slot released | n_cache_tokens=171 n_ctx=8192 n_past=171 n_system_tokens=0 slot_id=0 task_id=5 tid="0x1e9db3240" timestamp=1743653983 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/embedding" remote_addr="127.0.0.1" remote_port=62619 status=200 tid="0x16f9b3000" timestamp=1743653983
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=8 tid="0x1e9db3240" timestamp=1743653983
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=9 tid="0x1e9db3240" timestamp=1743653983
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=9 tid="0x1e9db3240" timestamp=1743653983
DEBUG [update_slots] slot released | n_cache_tokens=135 n_ctx=8192 n_past=135 n_system_tokens=0 slot_id=0 task_id=9 tid="0x1e9db3240" timestamp=1743653983 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/embedding" remote_addr="127.0.0.1" remote_port=62620 status=200 tid="0x16fa3f000" timestamp=1743653983
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=12 tid="0x1e9db3240" timestamp=1743653983
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=13 tid="0x1e9db3240" timestamp=1743653983
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=13 tid="0x1e9db3240" timestamp=1743653983
DEBUG [update_slots] slot released | n_cache_tokens=332 n_ctx=8192 n_past=332 n_system_tokens=0 slot_id=0 task_id=13 tid="0x1e9db3240" timestamp=1743653983 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/embedding" remote_addr="127.0.0.1" remote_port=62620 status=200 tid="0x16fa3f000" timestamp=1743653983
[GIN] 2025/04/03 - 09:49:43 | 200 |   658.69725ms |       127.0.0.1 | POST     "/api/embed"
time=2025-04-03T09:49:43.843+05:30 level=DEBUG source=sched.go:466 msg="context for request finished"
time=2025-04-03T09:49:43.844+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 duration=5m0s
time=2025-04-03T09:49:43.844+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 refCount=0
