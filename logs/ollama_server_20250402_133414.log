2025/04/02 13:34:16 routes.go:1158: INFO server config env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:true OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/harsh/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: http_proxy: https_proxy: no_proxy:]"
time=2025-04-02T13:34:16.332+05:30 level=INFO source=images.go:754 msg="total blobs: 25"
time=2025-04-02T13:34:16.335+05:30 level=INFO source=images.go:761 msg="total unused blobs removed: 0"
time=2025-04-02T13:34:16.335+05:30 level=INFO source=routes.go:1205 msg="Listening on 127.0.0.1:11434 (version 0.3.14)"
time=2025-04-02T13:34:16.342+05:30 level=INFO source=common.go:135 msg="extracting embedded files" dir=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama2878635254/runners
time=2025-04-02T13:34:16.342+05:30 level=DEBUG source=common.go:168 msg=extracting runner=metal payload=darwin/arm64/metal/ollama_llama_server.gz
time=2025-04-02T13:34:16.361+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama2878635254/runners/metal/ollama_llama_server
time=2025-04-02T13:34:16.361+05:30 level=INFO source=common.go:49 msg="Dynamic LLM libraries" runners=[metal]
time=2025-04-02T13:34:16.361+05:30 level=DEBUG source=common.go:50 msg="Override detection logic by setting OLLAMA_LLM_LIBRARY"
time=2025-04-02T13:34:16.361+05:30 level=DEBUG source=sched.go:105 msg="starting llm scheduler"
time=2025-04-02T13:34:16.404+05:30 level=INFO source=types.go:123 msg="inference compute" id=0 library=metal variant="" compute="" driver=0.0 name="" total="21.3 GiB" available="21.3 GiB"
time=2025-04-02T13:34:32.912+05:30 level=DEBUG source=sched.go:181 msg="updating default concurrency" OLLAMA_MAX_LOADED_MODELS=0x104e796d0 gpu_count=1
time=2025-04-02T13:34:32.921+05:30 level=DEBUG source=sched.go:224 msg="loading first model" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
time=2025-04-02T13:34:32.922+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-04-02T13:34:32.922+05:30 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 gpu=0 parallel=4 available=22906503168 required="6.2 GiB"
time=2025-04-02T13:34:32.925+05:30 level=INFO source=server.go:105 msg="system memory" total="32.0 GiB" free="11.2 GiB" free_swap="0 B"
time=2025-04-02T13:34:32.925+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-04-02T13:34:32.925+05:30 level=INFO source=memory.go:326 msg="offload to metal" layers.requested=-1 layers.model=41 layers.offload=41 layers.split="" memory.available="[21.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.2 GiB" memory.required.partial="6.2 GiB" memory.required.kv="2.5 GiB" memory.required.allocations="[6.2 GiB]" memory.weights.total="3.8 GiB" memory.weights.repeating="3.8 GiB" memory.weights.nonrepeating="78.8 MiB" memory.graph.full="1.6 GiB" memory.graph.partial="1.6 GiB"
time=2025-04-02T13:34:32.925+05:30 level=DEBUG source=common.go:168 msg=extracting runner=metal payload=darwin/arm64/metal/ollama_llama_server.gz
time=2025-04-02T13:34:32.925+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama2878635254/runners/metal/ollama_llama_server
time=2025-04-02T13:34:32.925+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama2878635254/runners/metal/ollama_llama_server
time=2025-04-02T13:34:32.926+05:30 level=INFO source=server.go:388 msg="starting llama server" cmd="/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama2878635254/runners/metal/ollama_llama_server --model /Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 --ctx-size 32384 --batch-size 512 --embedding --n-gpu-layers 41 --verbose --threads 8 --parallel 4 --port 56983"
time=2025-04-02T13:34:32.927+05:30 level=DEBUG source=server.go:405 msg=subprocess environment="[PATH=/Users/harsh/Desktop/Continue.dev-Granite-manual-test-cases/granite/bin:/opt/anaconda3/bin:/opt/homebrew/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/harsh/.local/bin LD_LIBRARY_PATH=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama2878635254/runners/metal]"
time=2025-04-02T13:34:32.928+05:30 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-04-02T13:34:32.928+05:30 level=INFO source=server.go:587 msg="waiting for llama runner to start responding"
time=2025-04-02T13:34:32.929+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server error"
INFO [main] starting c++ runner | tid="0x1e9db3240" timestamp=1743581073
INFO [main] build info | build=3871 commit="f37ceeaa" tid="0x1e9db3240" timestamp=1743581073
INFO [main] system info | n_threads=8 n_threads_batch=8 system_info="AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="0x1e9db3240" timestamp=1743581073 total_threads=10
INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="9" port="56983" tid="0x1e9db3240" timestamp=1743581073
llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from /Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = granite
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Granite 3.2 2b Instruct
llama_model_loader: - kv   3:                           general.finetune str              = instruct
llama_model_loader: - kv   4:                           general.basename str              = granite-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 2B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                   general.base_model.count u32              = 1
llama_model_loader: - kv   8:                  general.base_model.0.name str              = Granite 3.1 2b Instruct
llama_model_loader: - kv   9:          general.base_model.0.organization str              = Ibm Granite
llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/ibm-granite/gr...
llama_model_loader: - kv  11:                               general.tags arr[str,3]       = ["language", "granite-3.2", "text-gen...
llama_model_loader: - kv  12:                        granite.block_count u32              = 40
llama_model_loader: - kv  13:                     granite.context_length u32              = 131072
llama_model_loader: - kv  14:                   granite.embedding_length u32              = 2048
llama_model_loader: - kv  15:                granite.feed_forward_length u32              = 8192
llama_model_loader: - kv  16:               granite.attention.head_count u32              = 32
llama_model_loader: - kv  17:            granite.attention.head_count_kv u32              = 8
llama_model_loader: - kv  18:                     granite.rope.freq_base f32              = 5000000.000000
llama_model_loader: - kv  19:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  20:                          general.file_type u32              = 15
llama_model_loader: - kv  21:                         granite.vocab_size u32              = 49155
llama_model_loader: - kv  22:               granite.rope.dimension_count u32              = 64
llama_model_loader: - kv  23:                    granite.attention.scale f32              = 0.015625
llama_model_loader: - kv  24:                    granite.embedding_scale f32              = 12.000000
llama_model_loader: - kv  25:                     granite.residual_scale f32              = 0.220000
llama_model_loader: - kv  26:                        granite.logit_scale f32              = 8.000000
llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,49155]   = ["<|end_of_text|>", "<fim_prefix>", "...
llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  31:                      tokenizer.ggml.merges arr[str,48891]   = ["Ġ Ġ", "ĠĠ ĠĠ", "ĠĠĠĠ ĠĠ...
llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  33:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  34:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  36:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  37:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...
llama_model_loader: - kv  38:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  39:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   81 tensors
llama_model_loader: - type q4_K:  240 tensors
llama_model_loader: - type q6_K:   41 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.2826 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = granite
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 49155
llm_load_print_meta: n_merges         = 48891
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 40
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 512
llm_load_print_meta: n_embd_v_gqa     = 512
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 8.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 5000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 2.53 B
llm_load_print_meta: model size       = 1.44 GiB (4.87 BPW) 
llm_load_print_meta: general.name     = Granite 3.2 2b Instruct
llm_load_print_meta: BOS token        = 0 '<|end_of_text|>'
llm_load_print_meta: EOS token        = 0 '<|end_of_text|>'
llm_load_print_meta: UNK token        = 0 '<|end_of_text|>'
llm_load_print_meta: PAD token        = 0 '<|end_of_text|>'
llm_load_print_meta: LF token         = 145 'Ä'
llm_load_print_meta: EOG token        = 0 '<|end_of_text|>'
llm_load_print_meta: max token length = 512
llm_load_print_meta: f_embedding_scale = 12.000000
llm_load_print_meta: f_residual_scale  = 0.220000
llm_load_print_meta: f_attention_scale = 0.015625
llm_load_tensors: ggml ctx size =    0.34 MiB
ggml_backend_metal_log_allocated_size: allocated buffer, size =  1472.06 MiB, ( 1472.12 / 21845.34)
llm_load_tensors: offloading 40 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 41/41 layers to GPU
llm_load_tensors:        CPU buffer size =    78.75 MiB
llm_load_tensors:      Metal buffer size =  1472.05 MiB
llama_new_context_with_model: n_ctx      = 32384
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 5000000.0
llama_new_context_with_model: freq_scale = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M1 Pro
ggml_metal_init: picking default device: Apple M1 Pro
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M1 Pro
ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction support   = true
ggml_metal_init: simdgroup matrix mul. support = true
ggml_metal_init: hasUnifiedMemory              = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB
time=2025-04-02T13:34:33.686+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server loading model"
time=2025-04-02T13:34:33.686+05:30 level=DEBUG source=server.go:632 msg="model load progress 1.00"
time=2025-04-02T13:34:33.939+05:30 level=DEBUG source=server.go:635 msg="model load completed, waiting for server to become available" status="llm server loading model"
llama_kv_cache_init:      Metal KV buffer size =  2530.00 MiB
llama_new_context_with_model: KV self size  = 2530.00 MiB, K (f16): 1265.00 MiB, V (f16): 1265.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.78 MiB
llama_new_context_with_model:      Metal compute buffer size =  2103.25 MiB
llama_new_context_with_model:        CPU compute buffer size =    67.26 MiB
llama_new_context_with_model: graph nodes  = 1368
llama_new_context_with_model: graph splits = 2
DEBUG [initialize] initializing slots | n_slots=4 tid="0x1e9db3240" timestamp=1743581076
DEBUG [initialize] new slot | n_ctx_slot=8096 slot_id=0 tid="0x1e9db3240" timestamp=1743581076
DEBUG [initialize] new slot | n_ctx_slot=8096 slot_id=1 tid="0x1e9db3240" timestamp=1743581076
DEBUG [initialize] new slot | n_ctx_slot=8096 slot_id=2 tid="0x1e9db3240" timestamp=1743581076
DEBUG [initialize] new slot | n_ctx_slot=8096 slot_id=3 tid="0x1e9db3240" timestamp=1743581076
INFO [main] model loaded | tid="0x1e9db3240" timestamp=1743581076
DEBUG [update_slots] all slots are idle and system prompt is empty, clear the KV cache | tid="0x1e9db3240" timestamp=1743581076
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=0 tid="0x1e9db3240" timestamp=1743581076
time=2025-04-02T13:34:36.384+05:30 level=INFO source=server.go:626 msg="llama runner started in 3.46 seconds"
time=2025-04-02T13:34:36.384+05:30 level=DEBUG source=sched.go:462 msg="finished setting up runner" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=1 tid="0x1e9db3240" timestamp=1743581076
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=56992 status=200 tid="0x16f8e7000" timestamp=1743581076
time=2025-04-02T13:34:36.388+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|><important_rules>\n  Always include the language and file name in the info string when you write code blocks. If you are editing \"src/main.py\" for example, your code block should start with '```python src/main.py'.\n</important_rules>\n\nI am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>answer brief: What is AWS?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=2 tid="0x1e9db3240" timestamp=1743581076
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=3 tid="0x1e9db3240" timestamp=1743581076
DEBUG [update_slots] slot progression | ga_i=0 n_past=0 n_past_se=0 n_prompt_tokens_processed=161 slot_id=0 task_id=3 tid="0x1e9db3240" timestamp=1743581076
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=3 tid="0x1e9db3240" timestamp=1743581076
DEBUG [print_timings] prompt eval time     =     381.50 ms /   161 tokens (    2.37 ms per token,   422.02 tokens per second) | n_prompt_tokens_processed=161 n_tokens_second=422.01613621946933 slot_id=0 t_prompt_processing=381.502 t_token=2.3695776397515527 task_id=3 tid="0x1e9db3240" timestamp=1743581078
DEBUG [print_timings] generation eval time =    2112.39 ms /   108 runs   (   19.56 ms per token,    51.13 tokens per second) | n_decoded=108 n_tokens_second=51.126970992071534 slot_id=0 t_token=19.559148148148147 t_token_generation=2112.388 task_id=3 tid="0x1e9db3240" timestamp=1743581078
DEBUG [print_timings]           total time =    2493.89 ms | slot_id=0 t_prompt_processing=381.502 t_token_generation=2112.388 t_total=2493.89 task_id=3 tid="0x1e9db3240" timestamp=1743581078
DEBUG [update_slots] slot released | n_cache_tokens=269 n_ctx=32384 n_past=268 n_system_tokens=0 slot_id=0 task_id=3 tid="0x1e9db3240" timestamp=1743581078 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=56992 status=200 tid="0x16f8e7000" timestamp=1743581078
[GIN] 2025/04/02 - 13:34:38 | 200 |  5.980013333s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-02T13:34:38.883+05:30 level=DEBUG source=sched.go:466 msg="context for request finished"
time=2025-04-02T13:34:38.884+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 duration=30m0s
time=2025-04-02T13:34:38.884+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 refCount=0
time=2025-04-02T13:34:38.986+05:30 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=114 tid="0x1e9db3240" timestamp=1743581078
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=115 tid="0x1e9db3240" timestamp=1743581078
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=56996 status=200 tid="0x16f973000" timestamp=1743581078
time=2025-04-02T13:34:38.993+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|>I am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Given the following... please reply with a title for the chat that is 3-4 words in length, all words used should be directly related to the content of the chat, avoid using verbs unless they are directly related to the content of the chat, no additional text or explanation, you don't need ending punctuation.\n\nAWS (Amazon Web Services) is a comprehensive, evolving cloud computing platform provided by Amazon. It offers a mix of infrastructure as a service (IaaS), platform as a service (PaaS), and packaged software as a service (SaaS) offerings. This includes compute power for nearly any type of application deployment, storage options, networking capabilities, and additional managed services like database, machine learning, and serverless computing. AWS is designed to provide developers with on-demand cloud computing platforms and APIs to get started quickly.<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=116 tid="0x1e9db3240" timestamp=1743581078
DEBUG [prefix_slot] slot with common prefix found | 0=["slot_id",0,"characters",38]
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=117 tid="0x1e9db3240" timestamp=1743581078
DEBUG [update_slots] slot progression | ga_i=0 n_past=3 n_past_se=0 n_prompt_tokens_processed=267 slot_id=0 task_id=117 tid="0x1e9db3240" timestamp=1743581078
DEBUG [update_slots] kv cache rm [p0, end) | p0=3 slot_id=0 task_id=117 tid="0x1e9db3240" timestamp=1743581078
DEBUG [print_timings] prompt eval time     =     506.87 ms /   267 tokens (    1.90 ms per token,   526.76 tokens per second) | n_prompt_tokens_processed=267 n_tokens_second=526.7602077052984 slot_id=0 t_prompt_processing=506.872 t_token=1.8983970037453184 task_id=117 tid="0x1e9db3240" timestamp=1743581079
DEBUG [print_timings] generation eval time =     222.96 ms /    12 runs   (   18.58 ms per token,    53.82 tokens per second) | n_decoded=12 n_tokens_second=53.82252023951021 slot_id=0 t_token=18.579583333333336 t_token_generation=222.955 task_id=117 tid="0x1e9db3240" timestamp=1743581079
DEBUG [print_timings]           total time =     729.83 ms | slot_id=0 t_prompt_processing=506.872 t_token_generation=222.955 t_total=729.827 task_id=117 tid="0x1e9db3240" timestamp=1743581079
DEBUG [update_slots] slot released | n_cache_tokens=279 n_ctx=32384 n_past=278 n_system_tokens=0 slot_id=0 task_id=117 tid="0x1e9db3240" timestamp=1743581079 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=56996 status=200 tid="0x16f973000" timestamp=1743581079
[GIN] 2025/04/02 - 13:34:39 | 200 |  752.045916ms |       127.0.0.1 | POST     "/api/chat"
time=2025-04-02T13:34:39.725+05:30 level=DEBUG source=sched.go:407 msg="context for request finished"
time=2025-04-02T13:34:39.725+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 duration=30m0s
time=2025-04-02T13:34:39.725+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 refCount=0
[GIN] 2025/04/02 - 13:34:53 | 200 |      34.209µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/04/02 - 13:34:53 | 200 |      96.916µs |       127.0.0.1 | GET      "/api/ps"
time=2025-04-02T13:34:57.786+05:30 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=132 tid="0x1e9db3240" timestamp=1743581097
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=133 tid="0x1e9db3240" timestamp=1743581097
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57001 status=200 tid="0x16f9ff000" timestamp=1743581097
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=134 tid="0x1e9db3240" timestamp=1743581097
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57001 status=200 tid="0x16f9ff000" timestamp=1743581097
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=135 tid="0x1e9db3240" timestamp=1743581097
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57002 status=200 tid="0x16fa8b000" timestamp=1743581097
time=2025-04-02T13:34:57.797+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|><important_rules>\n  Always include the language and file name in the info string when you write code blocks. If you are editing \"src/main.py\" for example, your code block should start with '```python src/main.py'.\n</important_rules>\n\nI am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>answer brief: What is AWS?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>AWS (Amazon Web Services) is a comprehensive, evolving cloud computing platform provided by Amazon. It offers a mix of infrastructure as a service (IaaS), platform as a service (PaaS), and packaged software as a service (SaaS) offerings. This includes compute power for nearly any type of application deployment, storage options, networking capabilities, and additional managed services like database, machine learning, and serverless computing. AWS is designed to provide developers with on-demand cloud computing platforms and APIs to get started quickly.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>what is java<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=136 tid="0x1e9db3240" timestamp=1743581097
DEBUG [prefix_slot] slot with common prefix found | 0=["slot_id",0,"characters",38]
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=137 tid="0x1e9db3240" timestamp=1743581097
DEBUG [update_slots] slot progression | ga_i=0 n_past=3 n_past_se=0 n_prompt_tokens_processed=281 slot_id=0 task_id=137 tid="0x1e9db3240" timestamp=1743581097
DEBUG [update_slots] kv cache rm [p0, end) | p0=3 slot_id=0 task_id=137 tid="0x1e9db3240" timestamp=1743581097
DEBUG [print_timings] prompt eval time     =     727.31 ms /   281 tokens (    2.59 ms per token,   386.36 tokens per second) | n_prompt_tokens_processed=281 n_tokens_second=386.35519929603606 slot_id=0 t_prompt_processing=727.31 t_token=2.588291814946619 task_id=137 tid="0x1e9db3240" timestamp=1743581105
DEBUG [print_timings] generation eval time =    7398.60 ms /   352 runs   (   21.02 ms per token,    47.58 tokens per second) | n_decoded=352 n_tokens_second=47.57660069242876 slot_id=0 t_token=21.018735795454546 t_token_generation=7398.595 task_id=137 tid="0x1e9db3240" timestamp=1743581105
DEBUG [print_timings]           total time =    8125.91 ms | slot_id=0 t_prompt_processing=727.31 t_token_generation=7398.595 t_total=8125.905000000001 task_id=137 tid="0x1e9db3240" timestamp=1743581105
DEBUG [update_slots] slot released | n_cache_tokens=633 n_ctx=32384 n_past=632 n_system_tokens=0 slot_id=0 task_id=137 tid="0x1e9db3240" timestamp=1743581105 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=57002 status=200 tid="0x16fa8b000" timestamp=1743581105
[GIN] 2025/04/02 - 13:35:05 | 200 |  8.147648958s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-02T13:35:05.925+05:30 level=DEBUG source=sched.go:407 msg="context for request finished"
time=2025-04-02T13:35:05.925+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 duration=30m0s
time=2025-04-02T13:35:05.925+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 refCount=0
time=2025-04-02T13:35:43.907+05:30 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=492 tid="0x1e9db3240" timestamp=1743581143
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=493 tid="0x1e9db3240" timestamp=1743581143
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57014 status=200 tid="0x16fb17000" timestamp=1743581143
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=494 tid="0x1e9db3240" timestamp=1743581143
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57014 status=200 tid="0x16fb17000" timestamp=1743581143
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=495 tid="0x1e9db3240" timestamp=1743581143
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57015 status=200 tid="0x16fba3000" timestamp=1743581143
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=496 tid="0x1e9db3240" timestamp=1743581143
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57015 status=200 tid="0x16fba3000" timestamp=1743581143
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=497 tid="0x1e9db3240" timestamp=1743581143
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57016 status=200 tid="0x16fc2f000" timestamp=1743581143
time=2025-04-02T13:35:43.936+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|><important_rules>\n  Always include the language and file name in the info string when you write code blocks. If you are editing \"src/main.py\" for example, your code block should start with '```python src/main.py'.\n</important_rules>\n\nI am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>answer brief: What is AWS?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>AWS (Amazon Web Services) is a comprehensive, evolving cloud computing platform provided by Amazon. It offers a mix of infrastructure as a service (IaaS), platform as a service (PaaS), and packaged software as a service (SaaS) offerings. This includes compute power for nearly any type of application deployment, storage options, networking capabilities, and additional managed services like database, machine learning, and serverless computing. AWS is designed to provide developers with on-demand cloud computing platforms and APIs to get started quickly.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>what is java<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>Java is a high-level, class-based, object-oriented programming language developed by Sun Microsystems (now owned by Oracle Corporation), first released in 1995 as 'Oak'. Java's 'Write Once, Run Anywhere' mantra signifies its ability to compile the source code into bytecode that can be executed on any device with a Java Virtual Machine (JVM). Key features of Java include:\n\n1. Platform Independence: Code written in Java compiles into an intermediate form called bytecode, which can run on any device that has a JVM, regardless of the underlying hardware or operating system.\n\n2. Object-Oriented Programming: It is built upon object-oriented programming principles and uses \"classes\" as blueprints for objects. Objects have properties (attributes) and methods to manipulate these attributes.\n\n3. Robustness: Java is designed with a strong focus on security, reliability, performance, and simplicity in the design.\n\n4. Garbage Collection: Java includes automatic memory management through garbage collection, which automatically frees memory occupied by objects that are no longer being used.\n\n5. Rich API Library: It provides a vast standard library supporting numerous functionalities such as networking, file I/O, user interfaces, and more, making it versatile for diverse applications.\n\n6. Portability: Java source files can be written on one system and run on others that have the JVM without modification due to bytecode execution.\n\nJava is widely adopted for building enterprise-scale applications (like Android apps, web services, big data platforms, etc.), desktop applications, and more thanks to its rich feature set, strong community support, and extensive libraries.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>share <|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=498 tid="0x1e9db3240" timestamp=1743581143
DEBUG [prefix_slot] slot with common prefix found | 0=["slot_id",0,"characters",1493]
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=499 tid="0x1e9db3240" timestamp=1743581143
DEBUG [update_slots] slot progression | ga_i=0 n_past=366 n_past_se=0 n_prompt_tokens_processed=637 slot_id=0 task_id=499 tid="0x1e9db3240" timestamp=1743581143
DEBUG [update_slots] kv cache rm [p0, end) | p0=366 slot_id=0 task_id=499 tid="0x1e9db3240" timestamp=1743581143
DEBUG [print_timings] prompt eval time     =     772.72 ms /   637 tokens (    1.21 ms per token,   824.36 tokens per second) | n_prompt_tokens_processed=637 n_tokens_second=824.358566211393 slot_id=0 t_prompt_processing=772.722 t_token=1.2130643642072214 task_id=499 tid="0x1e9db3240" timestamp=1743581147
DEBUG [print_timings] generation eval time =    3100.04 ms /   141 runs   (   21.99 ms per token,    45.48 tokens per second) | n_decoded=141 n_tokens_second=45.48332810221298 slot_id=0 t_token=21.986078014184397 t_token_generation=3100.037 task_id=499 tid="0x1e9db3240" timestamp=1743581147
DEBUG [print_timings]           total time =    3872.76 ms | slot_id=0 t_prompt_processing=772.722 t_token_generation=3100.037 t_total=3872.759 task_id=499 tid="0x1e9db3240" timestamp=1743581147
DEBUG [update_slots] slot released | n_cache_tokens=778 n_ctx=32384 n_past=777 n_system_tokens=0 slot_id=0 task_id=499 tid="0x1e9db3240" timestamp=1743581147 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=57016 status=200 tid="0x16fc2f000" timestamp=1743581147
[GIN] 2025/04/02 - 13:35:47 | 200 |  3.910765625s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-02T13:35:47.810+05:30 level=DEBUG source=sched.go:407 msg="context for request finished"
time=2025-04-02T13:35:47.810+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 duration=30m0s
time=2025-04-02T13:35:47.810+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 refCount=0
[GIN] 2025/04/02 - 13:36:19 | 200 |    9.085625ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/02 - 13:36:19 | 200 |    9.228542ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/02 - 13:36:19 | 200 |   14.168042ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/04/02 - 13:36:19 | 200 |   23.198458ms |       127.0.0.1 | POST     "/api/show"
time=2025-04-02T13:38:28.371+05:30 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=643 tid="0x1e9db3240" timestamp=1743581308
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=644 tid="0x1e9db3240" timestamp=1743581308
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57150 status=200 tid="0x16f7cf000" timestamp=1743581308
time=2025-04-02T13:38:28.393+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|><important_rules>\n  Always include the language and file name in the info string when you write code blocks. If you are editing \"src/main.py\" for example, your code block should start with '```python src/main.py'.\n</important_rules>\n\nI am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Lambda <|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=645 tid="0x1e9db3240" timestamp=1743581308
DEBUG [prefix_slot] slot with common prefix found | 0=["slot_id",0,"characters",732]
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=646 tid="0x1e9db3240" timestamp=1743581308
DEBUG [update_slots] slot progression | ga_i=0 n_past=149 n_past_se=0 n_prompt_tokens_processed=156 slot_id=0 task_id=646 tid="0x1e9db3240" timestamp=1743581308
DEBUG [update_slots] kv cache rm [p0, end) | p0=149 slot_id=0 task_id=646 tid="0x1e9db3240" timestamp=1743581308
time=2025-04-02T13:38:34.448+05:30 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
time=2025-04-02T13:38:34.449+05:30 level=DEBUG source=sched.go:407 msg="context for request finished"
[GIN] 2025/04/02 - 13:38:34 | 200 |  6.086071708s |       127.0.0.1 | POST     "/api/chat"
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=57150 status=200 tid="0x16f7cf000" timestamp=1743581314
DEBUG [process_single_task] slot data | n_idle_slots=3 n_processing_slots=1 task_id=881 tid="0x1e9db3240" timestamp=1743581314
DEBUG [update_slots] slot released | n_cache_tokens=390 n_ctx=32384 n_past=389 n_system_tokens=0 slot_id=0 task_id=646 tid="0x1e9db3240" timestamp=1743581314 truncated=false
time=2025-04-02T13:38:34.469+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 refCount=1
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=884 tid="0x1e9db3240" timestamp=1743581314
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57154 status=200 tid="0x16f85b000" timestamp=1743581314
time=2025-04-02T13:38:34.473+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|>I am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Given the following... please reply with a title for the chat that is 3-4 words in length, all words used should be directly related to the content of the chat, avoid using verbs unless they are directly related to the content of the chat, no additional text or explanation, you don't need ending punctuation.\n\nHere's an example of a simple Lambda function written in Python (using `lambda` for anonymous functions). This function calculates the factorial of a number:\n\n\n\nIn this example, `factorial` is a nested `lambda` function. It uses recursion to calculate the factorial of an input number (denoted as `n`). The outer function `compute_factorial` simply calls and returns the result of applying `factorial` to the provided number.\n\nYou would use this code in your Python environment with proper imports and context, not directly from a file named \"src/main.py\". Here's how you'd call it:\n\n```python\n# main.py (for demonstration)\nfrom src import compute_factorial\n\nnum = 5\nprint(f\"Factorial of {num<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=885 tid="0x1e9db3240" timestamp=1743581314
DEBUG [prefix_slot] slot with common prefix found | 0=["slot_id",0,"characters",38]
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=886 tid="0x1e9db3240" timestamp=1743581314
DEBUG [update_slots] slot progression | ga_i=0 n_past=3 n_past_se=0 n_prompt_tokens_processed=326 slot_id=0 task_id=886 tid="0x1e9db3240" timestamp=1743581314
DEBUG [update_slots] kv cache rm [p0, end) | p0=3 slot_id=0 task_id=886 tid="0x1e9db3240" timestamp=1743581314
DEBUG [print_timings] prompt eval time     =     658.50 ms /   326 tokens (    2.02 ms per token,   495.06 tokens per second) | n_prompt_tokens_processed=326 n_tokens_second=495.0622852135829 slot_id=0 t_prompt_processing=658.503 t_token=2.0199478527607364 task_id=886 tid="0x1e9db3240" timestamp=1743581315
DEBUG [print_timings] generation eval time =     155.80 ms /     8 runs   (   19.48 ms per token,    51.35 tokens per second) | n_decoded=8 n_tokens_second=51.34656363122898 slot_id=0 t_token=19.4755 t_token_generation=155.804 task_id=886 tid="0x1e9db3240" timestamp=1743581315
DEBUG [print_timings]           total time =     814.31 ms | slot_id=0 t_prompt_processing=658.503 t_token_generation=155.804 t_total=814.307 task_id=886 tid="0x1e9db3240" timestamp=1743581315
DEBUG [update_slots] slot released | n_cache_tokens=334 n_ctx=32384 n_past=333 n_system_tokens=0 slot_id=0 task_id=886 tid="0x1e9db3240" timestamp=1743581315 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=57154 status=200 tid="0x16f85b000" timestamp=1743581315
[GIN] 2025/04/02 - 13:38:35 | 200 |  855.063167ms |       127.0.0.1 | POST     "/api/chat"
time=2025-04-02T13:38:35.288+05:30 level=DEBUG source=sched.go:407 msg="context for request finished"
time=2025-04-02T13:38:35.289+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 duration=30m0s
time=2025-04-02T13:38:35.289+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 refCount=0
time=2025-04-02T14:03:25.827+05:30 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=897 tid="0x1e9db3240" timestamp=1743582805
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=898 tid="0x1e9db3240" timestamp=1743582805
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57516 status=200 tid="0x16f8e7000" timestamp=1743582805
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=899 tid="0x1e9db3240" timestamp=1743582805
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57516 status=200 tid="0x16f8e7000" timestamp=1743582805
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=900 tid="0x1e9db3240" timestamp=1743582805
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=57517 status=200 tid="0x16f973000" timestamp=1743582805
time=2025-04-02T14:03:25.841+05:30 level=DEBUG source=routes.go:1422 msg="chat request" images=0 prompt="<|start_of_role|>system<|end_of_role|><important_rules>\n  Always include the language and file name in the info string when you write code blocks. If you are editing \"src/main.py\" for example, your code block should start with '```python src/main.py'.\n</important_rules>\n\nI am not WatsonX, my name is IBM watsonx Code Assistant. I am an AI coding assistant from IBM with deep knowledge and expertise in programming languages. I am a powerful, professional, and respectful coding assistant who will respond to questions that are relevant to software development and engineering. I use the Large Language Model from IBM Granite that is based on the transformer decoder architecture.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Lambda <|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>Here's an example of a simple Lambda function written in Python (using `lambda` for anonymous functions). This function calculates the factorial of a number:\n\n```python\n# src/main.py (Lambda function)\nfactorial = lambda n: 1 if n == 0 else n * factorial(n - 1)\n\ndef compute_factorial(num):\n    return factorial(num)\n```\n\nIn this example, `factorial` is a nested `lambda` function. It uses recursion to calculate the factorial of an input number (denoted as `n`). The outer function `compute_factorial` simply calls and returns the result of applying `factorial` to the provided number.\n\nYou would use this code in your Python environment with proper imports and context, not directly from a file named \"src/main.py\". Here's how you'd call it:\n\n```python\n# main.py (for demonstration)\nfrom src import compute_factorial\n\nnum = 5\nprint(f\"Factorial of {num<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>share <|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>"
DEBUG [process_single_task] slot data | n_idle_slots=4 n_processing_slots=0 task_id=901 tid="0x1e9db3240" timestamp=1743582805
DEBUG [prefix_slot] slot with common prefix found | 0=["slot_id",0,"characters",38]
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=902 tid="0x1e9db3240" timestamp=1743582805
DEBUG [update_slots] slot progression | ga_i=0 n_past=3 n_past_se=0 n_prompt_tokens_processed=392 slot_id=0 task_id=902 tid="0x1e9db3240" timestamp=1743582805
DEBUG [update_slots] kv cache rm [p0, end) | p0=3 slot_id=0 task_id=902 tid="0x1e9db3240" timestamp=1743582805
time=2025-04-02T14:03:31.189+05:30 level=DEBUG source=sched.go:407 msg="context for request finished"
[GIN] 2025/04/02 - 14:03:31 | 200 |  5.370554042s |       127.0.0.1 | POST     "/api/chat"
time=2025-04-02T14:03:31.189+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 duration=30m0s
time=2025-04-02T14:03:31.189+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4 refCount=0
DEBUG [log_server_request] request | method="POST" params={} path="/completion" remote_addr="127.0.0.1" remote_port=57517 status=200 tid="0x16f973000" timestamp=1743582811
DEBUG [update_slots] slot released | n_cache_tokens=556 n_ctx=32384 n_past=555 n_system_tokens=0 slot_id=0 task_id=902 tid="0x1e9db3240" timestamp=1743582811 truncated=false
time=2025-04-02T15:07:31.059+05:30 level=DEBUG source=sched.go:341 msg="timer expired, expiring to unload" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
time=2025-04-02T15:07:31.061+05:30 level=DEBUG source=sched.go:360 msg="runner expired event received" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
time=2025-04-02T15:07:31.063+05:30 level=DEBUG source=sched.go:375 msg="got lock to unload" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
time=2025-04-02T15:07:31.063+05:30 level=DEBUG source=server.go:1086 msg="stopping llama server"
time=2025-04-02T15:07:31.065+05:30 level=DEBUG source=server.go:1092 msg="waiting for llama server to exit"
time=2025-04-02T15:07:31.093+05:30 level=DEBUG source=server.go:1096 msg="llama server stopped"
time=2025-04-02T15:07:31.093+05:30 level=DEBUG source=sched.go:380 msg="runner released" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
time=2025-04-02T15:07:31.093+05:30 level=DEBUG source=sched.go:384 msg="sending an unloaded event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-36c09b22ee395b0a80053af776e65d8c34cc38ee47eb1d842c2578f0dacd10d4
time=2025-04-02T15:07:31.093+05:30 level=DEBUG source=sched.go:308 msg="ignoring unload event with no pending requests"
time=2025-04-02T19:00:35.190+05:30 level=DEBUG source=sched.go:224 msg="loading first model" model=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6
time=2025-04-02T19:00:35.191+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-04-02T19:00:35.191+05:30 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=0 parallel=1 available=22906503168 required="864.9 MiB"
time=2025-04-02T19:00:35.193+05:30 level=INFO source=server.go:105 msg="system memory" total="32.0 GiB" free="10.6 GiB" free_swap="0 B"
time=2025-04-02T19:00:35.193+05:30 level=DEBUG source=memory.go:103 msg=evaluating library=metal gpu_count=1 available="[21.3 GiB]"
time=2025-04-02T19:00:35.193+05:30 level=INFO source=memory.go:326 msg="offload to metal" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[21.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="864.9 MiB" memory.required.partial="864.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[864.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-04-02T19:00:35.193+05:30 level=DEBUG source=common.go:168 msg=extracting runner=metal payload=darwin/arm64/metal/ollama_llama_server.gz
time=2025-04-02T19:00:35.193+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama2878635254/runners/metal/ollama_llama_server
time=2025-04-02T19:00:35.193+05:30 level=DEBUG source=common.go:294 msg="availableServers : found" file=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama2878635254/runners/metal/ollama_llama_server
time=2025-04-02T19:00:35.195+05:30 level=INFO source=server.go:388 msg="starting llama server" cmd="/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama2878635254/runners/metal/ollama_llama_server --model /Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --embedding --n-gpu-layers 13 --verbose --threads 8 --parallel 1 --port 59507"
time=2025-04-02T19:00:35.195+05:30 level=DEBUG source=server.go:405 msg=subprocess environment="[PATH=/Users/harsh/Desktop/Continue.dev-Granite-manual-test-cases/granite/bin:/opt/anaconda3/bin:/opt/homebrew/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/harsh/.local/bin LD_LIBRARY_PATH=/var/folders/h5/6m1jwfrs0d907pzdgx0mp5f40000gn/T/ollama2878635254/runners/metal]"
time=2025-04-02T19:00:35.197+05:30 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-04-02T19:00:35.197+05:30 level=INFO source=server.go:587 msg="waiting for llama runner to start responding"
time=2025-04-02T19:00:35.197+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server error"
INFO [main] starting c++ runner | tid="0x1e9db3240" timestamp=1743600635
INFO [main] build info | build=3871 commit="f37ceeaa" tid="0x1e9db3240" timestamp=1743600635
INFO [main] system info | n_threads=8 n_threads_batch=8 system_info="AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | " tid="0x1e9db3240" timestamp=1743600635 total_threads=10
INFO [main] HTTP server listening | hostname="127.0.0.1" n_threads_http="9" port="59507" tid="0x1e9db3240" timestamp=1743600635
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: ggml ctx size =    0.10 MiB
ggml_backend_metal_log_allocated_size: allocated buffer, size =   260.88 MiB, (  260.94 / 21845.34)
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CPU buffer size =    44.72 MiB
llm_load_tensors:      Metal buffer size =   260.87 MiB
llama_new_context_with_model: n_ctx      = 8192
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 1000.0
llama_new_context_with_model: freq_scale = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M1 Pro
ggml_metal_init: picking default device: Apple M1 Pro
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M1 Pro
ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction support   = true
ggml_metal_init: simdgroup matrix mul. support = true
ggml_metal_init: hasUnifiedMemory              = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB
time=2025-04-02T19:00:35.449+05:30 level=INFO source=server.go:621 msg="waiting for server to become available" status="llm server loading model"
time=2025-04-02T19:00:35.450+05:30 level=DEBUG source=server.go:632 msg="model load progress 1.00"
llama_kv_cache_init:      Metal KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
llama_new_context_with_model:      Metal compute buffer size =    23.00 MiB
llama_new_context_with_model:        CPU compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 2
DEBUG [initialize] initializing slots | n_slots=1 tid="0x1e9db3240" timestamp=1743600635
DEBUG [initialize] new slot | n_ctx_slot=8192 slot_id=0 tid="0x1e9db3240" timestamp=1743600635
INFO [main] model loaded | tid="0x1e9db3240" timestamp=1743600635
DEBUG [update_slots] all slots are idle and system prompt is empty, clear the KV cache | tid="0x1e9db3240" timestamp=1743600635
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=0 tid="0x1e9db3240" timestamp=1743600635
time=2025-04-02T19:00:35.701+05:30 level=INFO source=server.go:626 msg="llama runner started in 0.50 seconds"
time=2025-04-02T19:00:35.702+05:30 level=DEBUG source=sched.go:462 msg="finished setting up runner" model=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=1 tid="0x1e9db3240" timestamp=1743600635
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=59509 status=200 tid="0x16d377000" timestamp=1743600635
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=2 tid="0x1e9db3240" timestamp=1743600635
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=59510 status=200 tid="0x16d403000" timestamp=1743600635
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=3 tid="0x1e9db3240" timestamp=1743600635
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=59510 status=200 tid="0x16d403000" timestamp=1743600635
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=4 tid="0x1e9db3240" timestamp=1743600635
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=5 tid="0x1e9db3240" timestamp=1743600635
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=5 tid="0x1e9db3240" timestamp=1743600635
DEBUG [update_slots] slot released | n_cache_tokens=135 n_ctx=8192 n_past=135 n_system_tokens=0 slot_id=0 task_id=5 tid="0x1e9db3240" timestamp=1743600635 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/embedding" remote_addr="127.0.0.1" remote_port=59510 status=200 tid="0x16d403000" timestamp=1743600635
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=8 tid="0x1e9db3240" timestamp=1743600635
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=9 tid="0x1e9db3240" timestamp=1743600635
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=9 tid="0x1e9db3240" timestamp=1743600635
DEBUG [update_slots] slot released | n_cache_tokens=171 n_ctx=8192 n_past=171 n_system_tokens=0 slot_id=0 task_id=9 tid="0x1e9db3240" timestamp=1743600635 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/embedding" remote_addr="127.0.0.1" remote_port=59511 status=200 tid="0x16d48f000" timestamp=1743600635
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=12 tid="0x1e9db3240" timestamp=1743600635
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=13 tid="0x1e9db3240" timestamp=1743600635
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=13 tid="0x1e9db3240" timestamp=1743600635
DEBUG [update_slots] slot released | n_cache_tokens=304 n_ctx=8192 n_past=304 n_system_tokens=0 slot_id=0 task_id=13 tid="0x1e9db3240" timestamp=1743600635 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/embedding" remote_addr="127.0.0.1" remote_port=59511 status=200 tid="0x16d48f000" timestamp=1743600635
[GIN] 2025/04/02 - 19:00:35 | 200 |  656.884417ms |       127.0.0.1 | POST     "/api/embed"
time=2025-04-02T19:00:35.838+05:30 level=DEBUG source=sched.go:466 msg="context for request finished"
time=2025-04-02T19:00:35.838+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 duration=5m0s
time=2025-04-02T19:00:35.838+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 refCount=0
time=2025-04-02T19:04:09.412+05:30 level=DEBUG source=sched.go:575 msg="evaluating already loaded" model=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=16 tid="0x1e9db3240" timestamp=1743600849
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=17 tid="0x1e9db3240" timestamp=1743600849
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=59528 status=200 tid="0x16d51b000" timestamp=1743600849
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=18 tid="0x1e9db3240" timestamp=1743600849
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=59528 status=200 tid="0x16d51b000" timestamp=1743600849
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=19 tid="0x1e9db3240" timestamp=1743600849
DEBUG [log_server_request] request | method="POST" params={} path="/tokenize" remote_addr="127.0.0.1" remote_port=59529 status=200 tid="0x16d5a7000" timestamp=1743600849
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=20 tid="0x1e9db3240" timestamp=1743600849
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=21 tid="0x1e9db3240" timestamp=1743600849
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=21 tid="0x1e9db3240" timestamp=1743600849
DEBUG [update_slots] slot released | n_cache_tokens=171 n_ctx=8192 n_past=171 n_system_tokens=0 slot_id=0 task_id=21 tid="0x1e9db3240" timestamp=1743600849 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/embedding" remote_addr="127.0.0.1" remote_port=59529 status=200 tid="0x16d5a7000" timestamp=1743600849
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=24 tid="0x1e9db3240" timestamp=1743600849
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=25 tid="0x1e9db3240" timestamp=1743600849
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=25 tid="0x1e9db3240" timestamp=1743600849
DEBUG [update_slots] slot released | n_cache_tokens=135 n_ctx=8192 n_past=135 n_system_tokens=0 slot_id=0 task_id=25 tid="0x1e9db3240" timestamp=1743600849 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/embedding" remote_addr="127.0.0.1" remote_port=59530 status=200 tid="0x16d633000" timestamp=1743600849
DEBUG [process_single_task] slot data | n_idle_slots=1 n_processing_slots=0 task_id=28 tid="0x1e9db3240" timestamp=1743600849
DEBUG [launch_slot_with_data] slot is processing task | slot_id=0 task_id=29 tid="0x1e9db3240" timestamp=1743600849
DEBUG [update_slots] kv cache rm [p0, end) | p0=0 slot_id=0 task_id=29 tid="0x1e9db3240" timestamp=1743600849
DEBUG [update_slots] slot released | n_cache_tokens=320 n_ctx=8192 n_past=320 n_system_tokens=0 slot_id=0 task_id=29 tid="0x1e9db3240" timestamp=1743600849 truncated=false
DEBUG [log_server_request] request | method="POST" params={} path="/embedding" remote_addr="127.0.0.1" remote_port=59530 status=200 tid="0x16d633000" timestamp=1743600849
[GIN] 2025/04/02 - 19:04:09 | 200 |  141.863208ms |       127.0.0.1 | POST     "/api/embed"
time=2025-04-02T19:04:09.554+05:30 level=DEBUG source=sched.go:407 msg="context for request finished"
time=2025-04-02T19:04:09.554+05:30 level=DEBUG source=sched.go:339 msg="runner with non-zero duration has gone idle, adding timer" modelPath=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 duration=5m0s
time=2025-04-02T19:04:09.554+05:30 level=DEBUG source=sched.go:357 msg="after processing request finished event" modelPath=/Users/harsh/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 refCount=0
